{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geo Location"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Imports\n",
    "2. Paths\n",
    "3. Load and transform images\n",
    "4. Create and train model\n",
    "5. Extract image meta data\n",
    "6. Create dataframe\n",
    "7. Load to csv-file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sn; sn.set(font_scale=1.4)\n",
    "from sklearn.utils import shuffle           \n",
    "import matplotlib.pyplot as plt             \n",
    "import cv2                                 \n",
    "import tensorflow as tf                \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow.keras.utils as image2\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Input\n",
    "from PIL import Image\n",
    "import exifread\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_DIR_PATH = os.getcwd()\n",
    "\n",
    "# Training paths\n",
    "seg_train_path = CURR_DIR_PATH + \"//data//seg_train//seg_train\"\n",
    "\n",
    "train_forest_path = seg_train_path + \"//forest\"\n",
    "train_buildings_path = seg_train_path + \"//buildings\"\n",
    "train_glacier_path = seg_train_path + \"//glacier\"\n",
    "train_mountain_path = seg_train_path + \"//mountain\"\n",
    "train_sea_path = seg_train_path + \"//sea\"\n",
    "train_street_path = seg_train_path + \"//street\"\n",
    "\n",
    "# Test paths\n",
    "seg_test_path = CURR_DIR_PATH + \"//data//seg_test//seg_test\"\n",
    "\n",
    "test_forest_path = seg_test_path + \"//forest\"\n",
    "test_buildings_path = seg_test_path + \"//buildings\"\n",
    "test_glacier_path = seg_test_path + \"//glacier\"\n",
    "test_mountain_path = seg_test_path + \"//mountain\"\n",
    "test_sea_path = seg_test_path + \"//sea\"\n",
    "test_street_path = seg_test_path + \"//street\"\n",
    "\n",
    "# Prediction path\n",
    "\n",
    "seg_pred_path = CURR_DIR_PATH + \"//data//seg_pred//seg_pred\"\n",
    "\n",
    "# Our path\n",
    "\n",
    "seg_our_path = CURR_DIR_PATH + \"//data//seg_our//seg_our\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load and transform images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label = -1\n",
    "    for path in paths:\n",
    "        label = label+1\n",
    "        for file in tqdm(os.listdir(path)):\n",
    "            image_path = os.path.join(path, file)\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image,(150, 150)) \n",
    "            image = image/ 255.0\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "    images = np.array(images, dtype = 'float32')\n",
    "    labels = np.array(labels, dtype = 'int32')\n",
    "\n",
    "    output = (images,labels)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2191/2191 [00:04<00:00, 461.88it/s]\n",
      "100%|██████████| 2271/2271 [00:04<00:00, 495.89it/s]\n",
      "100%|██████████| 2404/2404 [00:04<00:00, 503.64it/s]\n",
      "100%|██████████| 2512/2512 [00:05<00:00, 467.56it/s]\n",
      "100%|██████████| 2274/2274 [00:04<00:00, 496.08it/s]\n",
      "100%|██████████| 2382/2382 [00:06<00:00, 356.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 14034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 437/437 [00:01<00:00, 345.08it/s]\n",
      "100%|██████████| 474/474 [00:01<00:00, 324.88it/s]\n",
      "100%|██████████| 553/553 [00:01<00:00, 464.01it/s]\n",
      "100%|██████████| 525/525 [00:01<00:00, 405.71it/s]\n",
      "100%|██████████| 510/510 [00:01<00:00, 423.16it/s]\n",
      "100%|██████████| 501/501 [00:01<00:00, 475.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing images: 3000\n"
     ]
    }
   ],
   "source": [
    "train_paths = [train_buildings_path, train_forest_path, train_glacier_path, train_mountain_path, train_sea_path, train_street_path]\n",
    "test_paths = [test_buildings_path, test_forest_path, test_glacier_path, test_mountain_path, test_sea_path, test_street_path]\n",
    "train_images, train_labels= load_data(train_paths)\n",
    "print (f\"Number of training images: {train_labels.shape[0]}\")\n",
    "test_images, test_labels = load_data(test_paths)\n",
    "print (f\"Number of testing images: {test_labels.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = shuffle(train_images, train_labels, random_state=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (150, 150, 3)), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(6, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "88/88 [==============================] - 111s 1s/step - loss: 1.3639 - accuracy: 0.5057 - val_loss: 0.9577 - val_accuracy: 0.6480\n",
      "Epoch 2/4\n",
      "88/88 [==============================] - 108s 1s/step - loss: 0.7993 - accuracy: 0.7104 - val_loss: 0.7713 - val_accuracy: 0.6990\n",
      "Epoch 3/4\n",
      "88/88 [==============================] - 111s 1s/step - loss: 0.6287 - accuracy: 0.7755 - val_loss: 0.6943 - val_accuracy: 0.7538\n",
      "Epoch 4/4\n",
      "88/88 [==============================] - 110s 1s/step - loss: 0.4812 - accuracy: 0.8348 - val_loss: 0.6881 - val_accuracy: 0.7613\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, batch_size=128, epochs=40, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gps_info(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        tags = exifread.process_file(image_file)\n",
    "        lat_deg = tags.get('GPS GPSLatitude', None)\n",
    "        lat_ref = tags.get('GPS GPSLatitudeRef', None)\n",
    "        long_deg = tags.get('GPS GPSLongitude', None)\n",
    "        long_ref = tags.get('GPS GPSLongitudeRef', None)\n",
    "        timestamp = tags.get('EXIF DateTimeOriginal', None)\n",
    "        latitude = convert_to_degrees(lat_deg, lat_ref)\n",
    "        longitude = convert_to_degrees(long_deg, long_ref)\n",
    "        return (latitude, longitude, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_degrees(coordinate, ref):\n",
    "    coordinate = list(coordinate.values)\n",
    "    d = float(coordinate[0].num) / coordinate[0].den\n",
    "    m = float(coordinate[1].num) / coordinate[1].den\n",
    "    s = float(coordinate[2].num) / coordinate[2].den\n",
    "    result = d + (m / 60.0) + (s / 3600.0)\n",
    "    if str(ref) == 'S' or str(ref) == 'W':\n",
    "        result *= -1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_name(latitude, longitude):\n",
    "    url = f\"https://nominatim.openstreetmap.org/reverse?format=json&lat={latitude}&lon={longitude}\"\n",
    "    response = requests.get(url)\n",
    "    location_name = response.json().get('address').get('town') or response.json().get('address').get('city')\n",
    "    country = response.json().get('address').get('country')\n",
    "    return location_name, country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(path):\n",
    "    img = cv2.imread(path)\n",
    "    test1_proc = image2.load_img(path , target_size=(150,150))\n",
    "    test1_proc_1 = image2.img_to_array(test1_proc)\n",
    "    t1 = np.expand_dims(test1_proc ,axis=0)\n",
    "    t1 = t1/255\n",
    "    pred_classes = model.predict(t1)\n",
    "    pred = np.argmax(pred_classes ,axis=1)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    pic = Image.open(path) \n",
    "    plt.xlabel(\"Original Image\")\n",
    "    plt.imshow(pic)\n",
    "    \n",
    "    \n",
    "    plt.subplot(122)\n",
    "\n",
    "    s1 = pd.Series(pred_classes.ravel() , index = category_label)\n",
    "    s1.plot(kind = 'bar' ,figsize = (10,5))\n",
    "    plt.xlabel(\"prediction\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extract image meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Possibly corrupted field Tag 0x001A in MakerNote IFD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Possibly corrupted field Tag 0x001A in MakerNote IFD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "longitudes = []\n",
    "latitudes = []\n",
    "timestamps = []\n",
    "location_names = []\n",
    "countries = []\n",
    "categories = []\n",
    "file_names = []\n",
    "category_label=['buildings','forest', 'glacier','mountain','sea','street']\n",
    "\n",
    "\n",
    "for file in os.listdir(seg_our_path):\n",
    "    image_path = (os.path.join(seg_our_path, file))\n",
    "    image_paths.append(image_path)\n",
    "    img = cv2.imread(image_path)\n",
    "    test1_proc = image2.load_img(image_path , target_size=(150,150))\n",
    "    test1_proc_1 = image2.img_to_array(test1_proc)\n",
    "    t1 = np.expand_dims(test1_proc ,axis=0)\n",
    "    t1 = t1/255\n",
    "    pred_classes = model.predict(t1)\n",
    "    pred = np.argmax(pred_classes ,axis=1)\n",
    "    for i in range(len(category_label)):\n",
    "        if pred == i:\n",
    "            category = category_label[i]\n",
    "    location = extract_gps_info(image_path)\n",
    "    if location:\n",
    "\n",
    "        longitude = location[1]\n",
    "        latitude = location[0]\n",
    "        location_name, country = get_location_name(location[0], location[1])\n",
    "        timestamp = location[2]\n",
    "        longitudes.append(longitude)\n",
    "        latitudes.append(latitude)\n",
    "        timestamps.append(timestamp)\n",
    "        location_names.append(location_name)\n",
    "        countries.append(country)\n",
    "        file_names.append(file)\n",
    "        categories.append(category)\n",
    "    else:\n",
    "        print(\"\\nNo geolocation information found in the photo.\" + file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             name   latitude  longitude                   country  \\\n",
      "0       berga.JPG  59.478528  18.315733                   Sverige   \n",
      "1      cinisi.JPG  38.170647  13.107097                    Italia   \n",
      "2       dubai.JPG  25.015797  55.725150  الإمارات العربية المتحدة   \n",
      "3      gdansk.JPG  54.382308  18.636897                    Polska   \n",
      "4    IMG-0562.JPG  60.127456  19.930411           Suomi / Finland   \n",
      "..            ...        ...        ...                       ...   \n",
      "111      nice.JPG  43.694325   7.261458                    France   \n",
      "112     norge.JPG  59.033683   6.593531                     Norge   \n",
      "113      prag.JPG  50.092025  14.406422                      None   \n",
      "114     skane.JPG  55.337589  13.347958                   Sverige   \n",
      "115     sthlm.JPG  59.345469  18.032781                   Sverige   \n",
      "\n",
      "               location                 time classification  \n",
      "0            Åkersberga  2020:11:07 17:20:51      buildings  \n",
      "1                Cinisi  2022:06:19 19:41:52         street  \n",
      "2                المدام  2017:12:08 09:39:07            sea  \n",
      "3                Gdańsk  2021:12:31 10:32:38      buildings  \n",
      "4                Jomala  2015:05:24 06:28:00        glacier  \n",
      "..                  ...                  ...            ...  \n",
      "111                Nice  2019:04:08 17:27:55         street  \n",
      "112                None  2022:08:15 11:04:17      buildings  \n",
      "113  Hlavní město Praha  2018:09:23 11:22:01       mountain  \n",
      "114                None  2022:08:25 12:32:09       mountain  \n",
      "115           Stockholm  2023:02:09 12:34:39      buildings  \n",
      "\n",
      "[116 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "image_dict = {'name': file_names, 'latitude':latitudes, 'longitude': longitudes, 'country':countries, 'location':location_names, 'time': timestamps, 'classification':categories}\n",
    "image_df = pd.DataFrame(image_dict)\n",
    "\n",
    "print(image_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Load to csv-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df.to_csv('image.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that latitude and longitude are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "-73.985625 40.748419444444444\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "longitudes = []\n",
    "latitudes = []\n",
    "timestamps = []\n",
    "location_names = []\n",
    "countries = []\n",
    "categories = []\n",
    "file_names = []\n",
    "category_label=['buildings','forest', 'glacier','mountain','sea','street']\n",
    "\n",
    "file2 = \"IMG_2064.jpg\"\n",
    "\n",
    "if True:\n",
    "    image_path = (os.path.join(seg_our_path, file2))\n",
    "    image_paths.append(image_path)\n",
    "    img = cv2.imread(image_path)\n",
    "    test1_proc = image2.load_img(image_path , target_size=(150,150))\n",
    "    test1_proc_1 = image2.img_to_array(test1_proc)\n",
    "    t1 = np.expand_dims(test1_proc ,axis=0)\n",
    "    t1 = t1/255\n",
    "    pred_classes = model.predict(t1)\n",
    "    pred = np.argmax(pred_classes ,axis=1)\n",
    "    for i in range(len(category_label)):\n",
    "        if pred == i:\n",
    "            category = category_label[i]\n",
    "    location = extract_gps_info(image_path)\n",
    "    if location:\n",
    "\n",
    "        longitude = location[1]\n",
    "        latitude = location[0]\n",
    "        print(longitude, latitude)\n",
    "        location_name, country = get_location_name(location[0], location[1])\n",
    "        timestamp = location[2]\n",
    "        longitudes.append(longitude)\n",
    "        latitudes.append(latitude)\n",
    "        timestamps.append(timestamp)\n",
    "        location_names.append(location_name)\n",
    "        countries.append(country)\n",
    "        file_names.append(file)\n",
    "        categories.append(category)\n",
    "    else:\n",
    "        print(\"\\nNo geolocation information found in the photo.\" + file2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
